\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{ragged2e}
\usepackage{subfigure}

\renewcommand{\raggedright}{\leftskip=0pt \rightskip=0pt plus 0cm}

\begin{document}
\centerline{\large{\textbf{CMPUT 366 Reading-Writing Exercise 3}}}

\ \\
\noindent \textbf{Name: Minghan Li}\\
\textbf{Student ID: 1561234}\\
\textbf{CCID: minghan4}\\

Chapter 4 introduces an efficient algorithm to solve the prediction and control problems in finite MDP, which is \textit{Dynamic Programming}. The Chapter breaks into three parts: solving prediction problem with DP, solving control problem with DP and efficiency of DP methods. \\

For prediction problems we use \textit{Policy Evaluation} algorithm, which computes a value function of the states based on the given policy using DP. Then for the control problem we have \textit{Policy Iteration} and \textit{Value Iteration}, and both methods fit into the \textit{General Policy Iteration} framework. \textit{Policy Iteration} consits of two parts: \textit{Policy Evaluation} and \textit{Policy Improvement}. These two methods form an alternating optimization process, which is guranteed to find an optimal value function and an optimal policy. In policy evaluation the algorithm do full sweep of the state until the value function converges, and in policy improvement we improve our policy by acting greedily with respect to the value function.\\

As for \textit{Value Iteration}, it use the biggest state-action value as back-up to update the state
value function, which is kind of mixing policy evaluation and policy improvement in one step. After the value function converges, then we can find a greedy policy which is also an optimal policy. Personally speaking, I think \textit{Policy Iteration} is more efficient than \textit{Value Iteration} in many cases. Because sometimes we just need to find an optimal policy rather than the whole optimal value function, and policies usually converge way faster than the value functions.\\

In addition, some people may think that prediction problems are much easier than control problem since our ultimate goal is to find the best policy, and \textit{Policy Evaluation} method is also used in the control problem. But this is not the case. As we mentioned above, for control problem we somtimes just have to find an optimal policy, which can occur even  when we don't have an opitmal value function;however, for policy evaluation we sometimes have to accurately compute the quality of each policy and return feedback to customers who provide those policies. Especially, for large and complex problems, \textit{Policy Evaluation} are often much harder than solving control problems.\\

Then we come to the \textit{General Policy Iteration}, which generalize the algorithms in \textit{Policy Iteration} and \textit{Value Iteration} and make them to be the two extreme special cases. Actually, as we are alternating between policy evalution and policy improvement, we can perform as many sweeps as we want for the value funcion before we update our policy, and it's guranteed to find the fixed point which satisfies \textit{Optimal Bellman Equation}. And for the last part the Chapter introduces \textit{Asynchrous DP} methods, which enable the algorithm to asynchronously update the value of each state, as long as it will visit all of them in the process.\\

Moreover, the reasons why Dynamic Programming can always find the optimal solution for the control probelms are that first, the Bellman Operator is a contration, so whenever we perform policy evaluation it's gurateed to converge to a fixed point; second, for policy improvement, the max operator make the function concave, so it also can find the best policy given the temporary value function. So if we are interleaving these two process togther we are just alternatingly optimizing both the policy and the value function, thus it will give us the optimal solutions.



\end{document}