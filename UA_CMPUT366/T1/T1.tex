\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage{ragged2e}
\usepackage{subfigure}

\renewcommand{\raggedright}{\leftskip=0pt \rightskip=0pt plus 0cm}

\begin{document}
\begin{center}
\begin{LARGE}
\ \\
\ \\

\bfseries{CMPUT 366 Reading-Writing Exercise 1}
\end{LARGE}
\ \\

\end{center}

\noindent \textbf{Minghan Li}\\
\textbf{Student ID: 1561234}\\
\textbf{CCID: minghan4}\\

\ \\
\noindent \textbf{Thought Question}
\ \\
The Reinforcement Learning algorithms introduced in Chapter 2 can perform quite well in the Bandit problems with appropriate hyperparameters. In fact most RL algorithms have so far achieved great performance on specific tasks, but in the non-stationary environments many of these methods seem to fail. Human-beings have the ability to quickly adapt to the fast-changing environment, so is it possible that we can also design an algorithm that allows agent to make flexible decisions in non-stationary environments or even in different task-settings? 


\ \\
\noindent \textbf{Answer 1: Meta Learning} 
\ \\
We can construct a meta-agent which can learn meta-policies from many other agents in different environments(or in the same non-stationary environment). Here a meta-polciy is about how to quickly adapt to different environments and how to adjust the policy when the environment is changing. So when the meta-agent perceives a change in the environment, the low level agent will receive a new policy generated by the meta-policy of the meta-agent. From my perspective, we can use the following approach to design such meta-agent:

\ \\
We are still using standard reinforcement learning setting(Markov Decision Process) and algorithms(e.g. Q-learning) for the meta-agent, but state space and action space are changed. A standard MDP consists of $<S, A, P, R>$, where $S$ is the set states and $A$ is set of actions. In the meta-agent case, features of the states can be vector that represents different environments and different policies in the non-stationary environments. The actions are choosing different policies given different states, which actully forms a meta-policy. Then the policy generated by the meta-policy can be used by a low level agent to quickly adapt to the environment. But since we have this hierarchy for agents, it's more difficult to do credit assignment, but I think that this still can be a feasible approach to solve the learning problem in non-stationary environment.

\ \\
\noindent \textbf{Answer 2: Learning more semantic representations} \ \\
Given a stationary environment, standard reinforcement learning algorithms can find an optimal policy in most time. But if the underlying MDP of the environment is changing, the optimal policy might then degrade to a sub-optimal one or even worse. To solve this, one might want to use more expressive models to learn the representations of the states, such as neural networks, which might have the ability to encode the pattern of changes in the environment. If we can learn more semantic representations rather than using primitive state features, the agent will be more informed and thus has a better chance to find an optimal solution in the non-stationary environments. However, one problem about expressive models such as neural networks is overfitting. Learning invariant representations is very hard in Reinforcement Learning, but I believe that it will be solved by researchers in the near future.

\end{document}